#!/usr/bin/env python
# Copyright 2025 Tobias Olenyi.
# SPDX-License-Identifier: Apache-2.0

"""
Utility script to clean up potentially problematic intermediary files
in the Snakemake pipeline, allowing rules to be re-run.

Currently, this script focuses on cleaning up failed PDB downloads.
It checks the status files generated by the pdb_download.py rule.
If a download was marked as unsuccessful, it deletes the corresponding
(likely empty or incomplete) .struct file. This allows Snakemake
to automatically retry downloading these files on the next run.
"""

import json
import logging
import os
from pathlib import Path
import sys
from typing import Dict

import yaml  # To read Snakemake config

# Basic logging configuration
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)],
)
logger = logging.getLogger(__name__)


def load_config(config_path: Path = Path("db_builder/config.yaml")) -> Dict:
    """
    Load the Snakemake configuration file.

    Args:
        config_path: Path to the configuration file. Defaults to "db_builder/config.yaml".

    Returns:
        A dictionary containing the configuration.

    Raises:
        SystemExit: If the config file is not found or cannot be parsed.
    """
    if not config_path.exists():
        logger.error(f"Configuration file not found: {config_path}")
        sys.exit(1)
    try:
        with open(config_path, "r") as f:
            config = yaml.safe_load(f)
        if not isinstance(config, dict):
            logger.error(f"Configuration file {config_path} is not a valid dictionary.")
            sys.exit(1)
        return config
    except yaml.YAMLError as e:
        logger.error(f"Error parsing configuration file {config_path}: {e}")
        sys.exit(1)
    except IOError as e:
        logger.error(f"Error reading configuration file {config_path}: {e}")
        sys.exit(1)


def clean_failed_pdb_downloads(pdb_dir: Path) -> int:
    """
    Identifies failed PDB downloads based on .status files and removes
    their corresponding .struct files.

    Args:
        pdb_dir: The directory containing the .struct and .status files.

    Returns:
        The number of .struct files cleaned.
    """
    if not pdb_dir.is_dir():
        logger.error(f"PDB directory not found or is not a directory: {pdb_dir}")
        return 0

    cleaned_count = 0
    status_files_found = 0
    expected_status_suffix = (
        ".status"  # Adjusted to handle potential .json suffix later
    )

    logger.info(f"Scanning for failed PDB downloads in: {pdb_dir}")

    # Iterate through all files, looking for status files
    for item in pdb_dir.iterdir():
        # Check if it's a status file (allow for .status or potentially .status.json)
        if item.is_file() and item.name.endswith(expected_status_suffix):
            status_files_found += 1
            # Extract PDB ID: remove the .status suffix
            pdb_id = item.name[: -len(expected_status_suffix)]

            # Define corresponding struct file path
            struct_file = pdb_dir / f"{pdb_id}.struct"

            try:
                with open(item, "r") as f:
                    status_data = json.load(f)

                if not status_data.get("success", False):
                    logger.info(
                        f"Found failed download status for PDB ID: {pdb_id} in {item.name}"
                    )
                    if struct_file.exists():
                        try:
                            os.remove(struct_file)
                            logger.info(
                                f"--> Deleted potentially incomplete file: {struct_file}"
                            )
                            cleaned_count += 1
                        except OSError as e:
                            logger.error(f"--> Failed to delete {struct_file}: {e}")
                    else:
                        # This is expected if the download failed before creating any file
                        logger.info(
                            f"--> Struct file {struct_file} not found (expected for failed download), nothing to delete."
                        )

            except json.JSONDecodeError:
                logger.warning(
                    f"Could not parse status file (may be corrupted): {item.name}. Skipping cleanup for this entry."
                )
                # Optionally: Treat as failure and attempt to delete struct file?
                # if struct_file.exists(): ...
            except IOError as e:
                logger.error(f"Could not read status file {item.name}: {e}")
            except Exception as e:
                logger.error(f"Unexpected error processing {item.name}: {e}")

    if status_files_found == 0:
        logger.warning(
            f"No {expected_status_suffix} files found in {pdb_dir}. Has the download step run?"
        )
    elif cleaned_count == 0:
        logger.info("No failed PDB downloads found requiring cleanup.")
    else:
        logger.info(
            f"PDB download cleanup finished. Deleted {cleaned_count} file(s) corresponding to failed downloads."
        )

    return cleaned_count


def clean_empty_foldseek_alignments(matrices_dir: Path) -> int:
    """
    Identifies and removes zero-byte Foldseek alignment (.aln) files.

    Args:
        matrices_dir: The directory containing the .aln files.

    Returns:
        The number of .aln files cleaned.
    """
    if not matrices_dir.is_dir():
        logger.warning(
            f"Matrices directory not found: {matrices_dir}. Skipping Foldseek alignment cleanup."
        )
        return 0

    cleaned_count = 0
    aln_files_found = 0
    expected_suffix = ".aln"

    logger.info(f"Scanning for empty Foldseek alignment files in: {matrices_dir}")

    for item in matrices_dir.iterdir():
        if item.is_file() and item.name.endswith(expected_suffix):
            aln_files_found += 1
            try:
                if item.stat().st_size == 0:
                    logger.info(f"Found empty alignment file: {item.name}")
                    try:
                        os.remove(item)
                        logger.info(f"--> Deleted empty file: {item}")
                        cleaned_count += 1
                    except OSError as e:
                        logger.error(f"--> Failed to delete {item}: {e}")
            except FileNotFoundError:
                # File might have been removed between listing and statting (unlikely but possible)
                logger.warning(f"Could not stat file {item}, it may have been removed.")
            except Exception as e:
                logger.error(f"Unexpected error processing {item.name}: {e}")

    if aln_files_found == 0:
        logger.info(
            f"No {expected_suffix} files found in {matrices_dir}. Has the alignment step run?"
        )
    elif cleaned_count > 0:
        logger.info(
            f"Foldseek alignment cleanup finished. Deleted {cleaned_count} empty {expected_suffix} file(s)."
        )
    else:
        logger.info("No empty Foldseek alignment files found requiring cleanup.")

    return cleaned_count


def clean_empty_domain_flags(domain_base_dir: Path) -> int:
    """
    Identifies and removes domain extraction flags (`extracted.flag`)
    for superfamilies where no actual domain (.cif) files were extracted.

    Args:
        domain_base_dir: The base directory containing superfamily subdirectories
                         (e.g., WORK_DIR/domains).

    Returns:
        The number of flag files cleaned.
    """
    if not domain_base_dir.is_dir():
        logger.warning(
            f"Domain base directory not found: {domain_base_dir}. Skipping domain flag cleanup."
        )
        return 0

    cleaned_count = 0
    flags_found = 0
    flag_filename = "extracted.flag"

    logger.info(
        f"Scanning for empty domain flags in subdirectories of: {domain_base_dir}"
    )

    # Iterate through potential superfamily subdirectories
    for sf_dir in domain_base_dir.iterdir():
        if sf_dir.is_dir():  # Check if it's actually a directory
            flag_file = sf_dir / flag_filename
            if flag_file.is_file():
                flags_found += 1
                try:
                    # Check if any .cif files exist in this superfamily directory
                    has_cif_files = any(sf_dir.glob("*.cif"))

                    if not has_cif_files:
                        logger.info(
                            f"Found flag file {flag_file} but no .cif files in {sf_dir}. Removing flag."
                        )
                        try:
                            os.remove(flag_file)
                            logger.info(f"--> Deleted empty flag file: {flag_file}")
                            cleaned_count += 1
                        except OSError as e:
                            logger.error(f"--> Failed to delete {flag_file}: {e}")
                    # else: Flag exists and CIF files exist - this is the expected valid state.

                except Exception as e:
                    logger.error(f"Unexpected error processing directory {sf_dir}: {e}")

    if flags_found == 0:
        logger.info(
            f"No {flag_filename} files found in subdirectories of {domain_base_dir}. Has the domain extraction step run?"
        )
    elif cleaned_count > 0:
        logger.info(
            f"Domain flag cleanup finished. Deleted {cleaned_count} flag file(s) corresponding to empty domain directories."
        )
    else:
        logger.info("No empty domain flags found requiring cleanup.")

    return cleaned_count


def main() -> None:
    """
    Main execution function. Loads config and runs cleanup tasks.
    """
    config = load_config()

    # Get base directories needed for path formatting
    # Provide defaults matching the snakefile if they are missing in config
    work_dir_base = Path(config.get("work_dir", "tmp/alignment_pipeline"))

    try:
        # Get specific paths from config, formatting them
        pdb_dir = Path(config["paths"]["pdb_files_dir"].format(work_dir=work_dir_base))
        matrices_dir = Path(
            config["paths"]["matrices_dir"].format(work_dir=work_dir_base)
        )
        domain_base_dir = Path(
            config["paths"]["domain_files_dir"].format(work_dir=work_dir_base)
        )
    except KeyError as e:
        logger.error(f"Missing expected path key in config['paths']: {e}")
        logger.error(
            "Ensure config.yaml contains pdb_files_dir, matrices_dir, and domain_files_dir under the paths section."
        )
        sys.exit(1)
    except Exception as e:
        logger.error(f"Error processing paths from config: {e}")
        sys.exit(1)

    logger.info("Starting pipeline cleanup...")

    # --- PDB Download Cleanup ---
    pdb_cleaned = clean_failed_pdb_downloads(pdb_dir)

    # --- Foldseek Alignment Cleanup ---
    aln_cleaned = clean_empty_foldseek_alignments(matrices_dir)

    # --- Domain Extraction Flag Cleanup ---
    flag_cleaned = clean_empty_domain_flags(domain_base_dir)

    # --- Add other cleanup functions here as needed ---
    # Example:
    # output_dir = Path(config.get("output_dir", "out/alignment_db"))
    # alignment_db_file = output_dir / "alignments.h5"
    # cleaned_outputs = clean_corrupted_outputs(alignment_db_file)

    total_cleaned = pdb_cleaned + aln_cleaned + flag_cleaned  # + other counts
    logger.info(f"Pipeline cleanup finished. Total files cleaned: {total_cleaned}")
    if total_cleaned > 0:
        logger.info("You can now re-run Snakemake to regenerate the cleaned steps.")


if __name__ == "__main__":
    main()
